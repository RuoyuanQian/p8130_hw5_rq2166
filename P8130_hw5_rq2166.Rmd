---
title: "p8130_hw5_rq2166"
author: "Ruoyuan Qian"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("faraway")
library(faraway)
library(tidyverse)
library(dplyr)
library(arsenal)
library(ggplot2)
library(ggpubr)
library(leaps)
#install.packages("devtools")
#devtools::install_github("thomasp85/patchwork")
#library(patchwork)
library(Rmisc)
library(faraway)
library(broom)
#install.packages(c('boot', 'caret','MPV'))     
library(boot)
library(caret)
library(MPV)
data(state.x77)
```

# Problem 1 

## a)
```{r}
state.x77 = 
  state.x77 %>% 
  as.data.frame() %>% 
<<<<<<< HEAD
  janitor::clean_names()
=======
  jean
>>>>>>> 5f0482fcbf6dd5a67c24fbdba86042d0dd5c55b3

 sum_data  <-  arsenal::tableby( ~ ., 
                                data  = state.x77,
                                test  = FALSE, 
                                total = FALSE,
                                numeric.stats =
                                  c("meansd","medianq1q3","range"))
summ = summary(sum_data,text = TRUE)
summ


```

## b)

```{r}
gg_p = 
    state.x77 %>% 
    ggplot(aes(x = population, y = life_exp))+
    geom_point()+
     labs(x = "population")
  
  
gg_in = 
    state.x77 %>% 
    ggplot(aes(x = income, y = life_exp))+
    geom_point()+
     labs(x = "income")
  
  
gg_il = 
    state.x77 %>% 
    ggplot(aes(x = illiteracy, y = life_exp))+
    geom_point()+
     labs(x = "illiteracy")
  
  
gg_m = 
    state.x77 %>% 
    ggplot(aes(x = murder, y = life_exp))+
    geom_point()+
     labs(x = "murder")
  
  
gg_h = 
    state.x77 %>% 
    ggplot(aes(x = hs_grad, y = life_exp))+
    geom_point()+
     labs(x = "hs_grad")
  
  
gg_f = 
    state.x77 %>% 
    ggplot(aes(x = frost, y = life_exp))+
    geom_point()+
     labs(x = "frost")
  

gg_a = 
    state.x77 %>% 
    ggplot(aes(x = area, y = life_exp))+
    geom_point()+
     labs(x = "area")


multiplot(gg_p,gg_in,gg_il,
          gg_m,gg_h,gg_f,gg_a,
          cols=2)  
```

```{r}
ggb_p = 
    state.x77 %>% 
    ggplot(aes(y = population))+
    geom_boxplot()+
     labs(y = "population")
  
  
ggb_in = 
    state.x77 %>% 
    ggplot(aes(y = income))+
    geom_boxplot()+
     labs(y = "income")
  
  
ggb_il = 
    state.x77 %>% 
    ggplot(aes(y = illiteracy))+
    geom_boxplot()+
     labs(y = "illiteracy")
  
  
ggb_m = 
    state.x77 %>% 
    ggplot(aes(y = murder))+
    geom_boxplot()+
     labs(y = "murder")
  
  
ggb_h = 
    state.x77 %>% 
    ggplot(aes(y = hs_grad))+
    geom_boxplot()+
     labs(y = "hs_grad")
  
  
ggb_f = 
    state.x77 %>% 
    ggplot(aes(y = frost))+
    geom_boxplot()+
     labs(y = "frost")
  

ggb_a = 
    state.x77 %>% 
    ggplot(aes(y = area))+
    geom_boxplot()+
     labs(y = "area")

ggb_li = 
    state.x77 %>% 
    ggplot(aes(y = life_exp))+
    geom_boxplot()+
     labs(y = "life_exp")


multiplot(ggb_p,ggb_in,ggb_il,
          ggb_m,ggb_h,ggb_f,ggb_a,ggb_li,
          cols=2)  
```

```{r}

ggh_p = 
state.x77 %>% 
 ggplot(aes(population,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "population")

ggh_in = 
state.x77 %>% 
 ggplot(aes(income,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "income")

ggh_il = 
state.x77 %>% 
 ggplot(aes(illiteracy,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "illiteracy")

ggh_m = 
state.x77 %>% 
 ggplot(aes(murder,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "murder")

ggh_h = 
state.x77 %>% 
 ggplot(aes(hs_grad,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "hs_grad")

ggh_f = 
state.x77 %>% 
 ggplot(aes(frost,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "frost")

ggh_a = 
state.x77 %>% 
 ggplot(aes(area,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "area")

ggh_li = 
state.x77 %>% 
 ggplot(aes(life_exp,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "life_exp")

multiplot(ggh_p,ggh_in,ggh_il,
          ggh_m,ggh_h,ggh_f,ggh_a,ggh_li,
          cols=2) 
```

Population and area are skewed

transformation
```{r}
ggl_p = 
state.x77 %>% 
  mutate(population = log(population)) %>% 
 ggplot(aes(population,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "population")

ggl_a = 
state.x77 %>% 
  mutate(area = log(area)) %>% 
 ggplot(aes(area,..density..))+
 geom_histogram()+
 geom_line(stat = 'density',size = 1)+
    labs(x = "area")


multiplot(ggl_p,ggl_a,
          cols=2) 

```

```{r}
state_log = 
state.x77 %>% 
  mutate(log_area = log(area),
         log_population = log(population))%>%  
  select(-area,-population)
```


## c)

### i

Correlation between murder and illiteracy is > 0.7.

backwards
```{r}

# Same thing
mult.fit <- lm(life_exp ~ ., data=state_log)
summary(mult.fit)

step1<-update(mult.fit, . ~ . -income)
summary(step1)

# No Alcmod
step2<-update(step1, . ~ . -illiteracy)
summary(step2)

# No Age
step3<-update(step2, . ~ . -log_area)
summary(step3)


```

forwards
```{r}
### Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
fit1 <- lm(life_exp ~ income, data=state_log)
tidy(fit1)
fit2 <- lm(life_exp ~ illiteracy, data=state_log)
tidy(fit2)
fit3 <- lm(life_exp ~ murder, data=state_log)
tidy(fit3)
fit4 <- lm(life_exp ~ hs_grad, data=state_log)
tidy(fit4)
fit5 <- lm(life_exp ~ frost, data=state_log)
tidy(fit5)
fit6 <- lm(life_exp ~ log_area, data=state_log)
tidy(fit6)
fit7 <- lm(life_exp ~ log_population, data=state_log)
tidy(fit7)

forward1<-lm(life_exp~murder, data=state_log)
tidy(forward1)

### Step 2: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward1, . ~ . +income)
tidy(fit1)
fit2 <- update(forward1, . ~ . +illiteracy)
tidy(fit2)
fit3 <- update(forward1, . ~ . +hs_grad)
tidy(fit3)
fit4 <- update(forward1, . ~ . +frost)
tidy(fit4)
fit5 <- update(forward1, . ~ . +log_area)
tidy(fit5)
fit6 <- update(forward1, . ~ . +log_population)
tidy(fit6)

# Enter the one with the lowest p-value: Progindex
forward2 <- update(forward1, . ~ . + hs_grad)
tidy(forward2)

### Step 3: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward2, . ~ . +income)
tidy(fit1)
fit2 <- update(forward2, . ~ . +illiteracy)
tidy(fit2)
fit3 <- update(forward2, . ~ . +frost)
tidy(fit3)
fit4 <- update(forward2, . ~ . +log_area)
tidy(fit4)
fit5 <- update(forward2, . ~ . +log_population)
tidy(fit5)

# Enter the one with the lowest p-value: Alcheav
forward3 <- update(forward2, . ~ . + log_population)
tidy(forward3)

### Step 4: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward3, . ~ . +income)
tidy(fit1)
fit2 <- update(forward3, . ~ . +illiteracy)
tidy(fit2)
fit3 <- update(forward3, . ~ . +frost)
tidy(fit3)
fit4 <- update(forward3, . ~ . +log_area)
tidy(fit4)

# Enter the one with the lowest p-value: Bloodclot
forward4 <- update(forward3, . ~ . + frost)
tidy(forward4)


### Step 5: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward4, . ~ . +income)
tidy(fit1)
fit2 <- update(forward4, . ~ . +illiteracy)
tidy(fit2)
fit3 <- update(forward4, . ~ . +log_area)
tidy(fit3)


# The model we obtained is Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot
for.fit <- lm(life_exp ~ murder + hs_grad + log_population + frost,state_log)
summary(for.fit)
```

stepwise
```{r}
mult.fit <- lm(life_exp ~ ., data=state_log)
step(mult.fit, direction='backward')
```

Yes, they generate the same model

### ii

`frost` is 0.042779, slightly less than 0.05, I decided to keep it since according to all of the three models,  they all contain the `frost`. Furthermore, the `AIC` is the smallest when select the `frost` into the model.


### iii

```{r}
cor(state_log)
influence.measures(for.fit)
par(mfrow=c(1,2))
plot(for.fit,which=4)
plot(for.fit,which=5)
```

```{r}
state_log_new = 
state_log %>% 
  filter(!(frost == 0))

for.fit <- lm(life_exp ~ murder + hs_grad + log_population + frost,state_log_new)
summary(for.fit)
```


The correlation between two is -0.657, so there is some association between them but not very strong.

## d)
```{r}
# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = state_log[,-3], y = state_log[,3], nbest=2, method="Cp")


# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = state_log[,-3], y = state_log[,3], nbest=2, method="adjr2")

# Summary of models for each size (one model per size)
# Function regsubsets() performs a subset slection by identifying the "best" model that contains
# a certain number of predictors. By default "best" is chosen using SSE/RSS (smaller is better).


b<-regsubsets(life_exp ~ ., data=state_log)
   (rs<-summary(b))

# This function also returns R2, Cp, BIC for each "best" model.
# Let's take a look at these values.

# Plots of Cp and Adj-R2 as functions of parameters

par(mar=c(4,4,1,1))
par(mfrow=c(1,2))

plot(2:8, rs$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1)

plot(2:8, rs$adjr2, xlab="No of parameters", ylab="Adj R2")


```

When the number of parameter is equal to 5, the largest Radj and smallest cp.

When the number of parameter is equal to 4, similar cp = number of parameters.

When the number of parameter is equal to 3, the most similar cp = number of parameters.

```{r}
# AIC of the 3-predictor model:

aic.fit3 <- lm(life_exp ~ murder + hs_grad + log_population,state_log )
summary(aic.fit3)
AIC(aic.fit3)

aic.fit4 <- lm(life_exp ~ murder + hs_grad + log_population + frost ,state_log )
summary(aic.fit4)
AIC(aic.fit4)

aic.fit5 <- lm(life_exp ~ murder + hs_grad + log_population + frost +log_area ,state_log )
summary(aic.fit5)
AIC(aic.fit5)
```

When 4 predictors, the AIC is the smallest. So choose model with 4 predictors.

murder + hs_grad + log_population + frost


## e)

### i

c) and d) get te same model.

```{r}
influence.measures(aic.fit4) 
```

There are four influencial points: Alaska, California, Hawaii, Nevada

We need more information about these four stats and we cannot delete them without any researches.

### ii
```{r}
par(mfrow = c(1, 2))
qqnorm(resid(aic.fit4), xlab = "Expected Value", ylab = "Residual", main = "")
qqline(resid(aic.fit4))
title("QQ Plot for Y (Life expendency)")


plot(fitted(aic.fit4), resid(aic.fit4), xlab = "Predicted/Fitted value", ylab = "Residual")
title("Residual Plot for lnY Life expendency)")
abline(0, 0)

par(mfrow=c(2,2))
plot(aic.fit4)

```

1. According to the QQ plot, the points are linear, so the residuals follow normal distribution.

2. According to the residual vs fitted values, the points are bounce arround y=0, which means they contain constant variance and they are independent to each other.

### iii

```{r}

# Use 5-fold validation and create the training sets

set.seed(1)
data_train<-trainControl(method="cv", number=10)

# Fit the 4-variables model that we discussed in previous lectures
model_caret<-train(life_exp ~ murder + hs_grad + log_population + frost,
                   data=state_log,
                   trControl=data_train,
                   method='lm',
                   na.action=na.pass)
  
# Model predictions using 4 parts of the data for training 
model_caret


# Model coefficients
model_caret$finalModel

# Examine model prediction for each fold
model_caret$resample

# Look at standard deviation around the Rsquared value by examining the R-squared from each fold.
sd(model_caret$resample$Rsquared)



# Let's look at the model using all data, no CV
full_model<-lm(life_exp ~ murder + hs_grad + log_population + frost, data=state_log)
summary(full_model)

# Full data had an Rsquared=0.8299; what was the CV result for R-squared?
# What about MSE values? Values are comparable.
```

The R squared(0.7674965) is higher than the whole model(0.7404).

## f)

R-squared, AIC, Cp

# Porblem 2

```{r}
CP = read_csv(".\\data\\CommercialProperties.csv") 

```


## a)

```{r}
mult.fit <- lm(Rental_rate ~ ., data=CP)
summary(mult.fit)
```

Except for the `Vacancy_rate`, all other variables are significant.

The R squared is 0.5629, which means 58.47% of the variance in y (Rental_rate) is explained by 4 variables(Age, Taxes, Vacancy_rate, Sq_footage).

## b)

```{r}

gg_a = 
    CP %>% 
    ggplot(aes(x = Age, y = Rental_rate))+
    geom_point()+
     labs(x = "Age")
  
  
gg_t = 
    CP %>% 
    ggplot(aes(x = Taxes, y = Rental_rate))+
    geom_point()+
     labs(x = "Taxes")
  
  
gg_s = 
    CP %>% 
    ggplot(aes(x = Sq_footage, y = Rental_rate))+
    geom_point()+
     labs(x = "Sq_footage")
  


multiplot(gg_a,gg_t,gg_s,
          cols=2)  
```

In the plots of `Sq_footage` and `Taxes`, there is a strong relation between `Sq_footage`, `Taxes` and the outcome(`Rental_rate`), respectively.

However, as for the Age plot, the points are diveided into two cluster apparently, and the linear association is not as strong as others.

## c)

```{r}
mult.fit <- lm(Rental_rate ~ Age + Taxes + Sq_footage, data=CP)
summary(mult.fit)
```

## d)

### i
```{r}
CP_center = 
CP %>% 
  mutate(Age_center = Age - mean(Age)) 
mult.fit <- lm(Rental_rate ~ Age + Taxes + Sq_footage + I(Age_center^2), data=CP_center)
summary(mult.fit)


mutate(CP_center, fitted = fitted(mult.fit)) %>%
  ggplot(., aes(y=Rental_rate, x=Age)) + geom_point() + 
  geom_line(aes(y = fitted), color = "red") + theme_bw()


```

The points from Age vs Rental_rate is not quadratic, so the higher order may not suitable, though the second order Age is significant.

We should centered Age, since it can avoid the correlation between first and second order of Age variable.

### ii

```{r}

CP$Ageest <- ifelse(CP$Age<8, 0, CP$Age-8)
#check<- cbind(data_hosp$NURSE,data_hosp$NURSEstar)

reg_spline<-lm(Rental_rate ~ Age + Taxes + Sq_footage + Ageest, data=CP)
summary(reg_spline)

mutate(CP, fitted = fitted(reg_spline)) %>%
  ggplot(., aes(y=Rental_rate, x=Age)) + geom_point() + 
  geom_line(aes(y = fitted), color = "red") + theme_bw()

reg_spline1<-lm(Rental_rate ~ Age + Ageest, data=CP)
summary(reg_spline1)

mutate(CP, fitted = fitted(reg_spline1)) %>%
  ggplot(., aes(y=Rental_rate, x=Age)) + geom_point() + 
  geom_line(aes(y = fitted), color = "red") + theme_bw()
```

According to the scatter plot, I split the points with Age = 8, since there are clearly two clusters in the plot.

### iii

The adj_R squared and SE are similar between two models, I recommend to use picewise, since it is easier to interpret 

## e)
```{r}

par(mfrow=c(2,2))
plot(reg_spline)

par(mfrow=c(2,2))
plot(mult.fit)


AIC(mult.fit)
AIC(reg_spline)



```

